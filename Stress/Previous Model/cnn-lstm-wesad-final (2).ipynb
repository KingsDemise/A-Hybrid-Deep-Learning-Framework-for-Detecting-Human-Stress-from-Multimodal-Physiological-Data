{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-20T12:12:27.419927Z","iopub.status.busy":"2024-01-20T12:12:27.419555Z","iopub.status.idle":"2024-01-20T12:12:27.803311Z","shell.execute_reply":"2024-01-20T12:12:27.802304Z","shell.execute_reply.started":"2024-01-20T12:12:27.419897Z"},"id":"xJMe3zF5y9iW","trusted":true},"outputs":[],"source":["import os\n","import pickle\n","import numpy as np\n","import pandas as pd\n","\n","# # Define the path to the dataset\n","# dataset_path = \"WESAD\"\n","\n","# # Loop through subjects from s2 to s17, excluding s12\n","# for subject_id in range(2, 18):\n","#     if subject_id == 12:\n","#         continue  # Skip s12\n","\n","#     # Construct the path to the subject data\n","#     subject_path = os.path.join(dataset_path, f\"S{subject_id}\")\n","#     subject_data_path = os.path.join(subject_path, f\"S{subject_id}.pkl\")\n","\n","#     # Load the subject data\n","#     with open(subject_data_path, 'rb') as file:\n","#         subject_data = pickle.load(file, encoding='latin1')\n","\n","#     # Extract features (modify this part based on your feature extraction logic)\n","#     chest_ax = subject_data['signal']['chest']['ACC'][:, 0]\n","#     chest_ay = subject_data['signal']['chest']['ACC'][:, 1]\n","#     chest_az = subject_data['signal']['chest']['ACC'][:, 2]\n","#     chest_ecg = subject_data['signal']['chest']['ECG'][:, 0]\n","#     chest_emg = subject_data['signal']['chest']['EMG'][:, 0]\n","#     chest_eda = subject_data['signal']['chest']['EDA'][:, 0]\n","#     chest_temp = subject_data['signal']['chest']['Temp'][:, 0]\n","#     chest_resp = subject_data['signal']['chest']['Resp'][:, 0]\n","\n","#     # Assuming label information is available, replace 'label_placeholder' with actual label data\n","#     label = subject_data['label']\n","\n","#     # Create a DataFrame\n","#     numpy_data = np.array([chest_ax, chest_ay, chest_az, chest_ecg, chest_emg, chest_eda, chest_temp, chest_resp, label])\n","#     numpy_data = numpy_data.T\n","#     df_subject = pd.DataFrame(data=numpy_data, columns=[\"chest_ax\", \"chest_ay\", \"chest_az\", \"chest_ecg\", \"chest_emg\", \"chest_eda\", \"chest_temp\", \"chest_resp\", \"label\"])\n","\n","#     # Output to CSV\n","#     output_csv_path = f'WESAD/subject_{subject_id}_data.csv'\n","#     df_subject.to_csv(output_csv_path, index=False)\n","\n","#     print(f\"Feature extraction and CSV output for subject {subject_id} completed.\")\n","\n","# print(\"Feature extraction and CSV output for all subjects completed.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-20T12:12:27.806074Z","iopub.status.busy":"2024-01-20T12:12:27.805196Z","iopub.status.idle":"2024-01-20T12:12:40.149742Z","shell.execute_reply":"2024-01-20T12:12:40.148763Z","shell.execute_reply.started":"2024-01-20T12:12:27.806035Z"},"id":"SHXOP3lvU-51","trusted":true},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, Dropout, Dense\n","from sklearn.preprocessing import LabelEncoder\n","from tensorflow.keras.utils import to_categorical\n","from sklearn.metrics import confusion_matrix\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import roc_curve, auc, roc_auc_score\n","import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, Dropout, Dense\n","from sklearn.preprocessing import LabelEncoder\n","from tensorflow.keras.utils import to_categorical\n","from sklearn.metrics import confusion_matrix\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import roc_curve, auc, roc_auc_score\n","from keras.optimizers import Adam\n","import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv1D, MaxPooling1D, GRU, Dropout, Dense, Flatten\n","from sklearn.preprocessing import LabelEncoder\n","from tensorflow.keras.utils import to_categorical\n","from sklearn.metrics import confusion_matrix\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import roc_curve, auc, roc_auc_score\n","from sklearn.model_selection import TimeSeriesSplit\n","# # Load preprocessed data\n","# data_paths = [\n","# #     \"/kaggle/input/csv-wesad/subject_3_data.csv\",\n","#     \"/kaggle/input/csv-wesad/subject_4_data.csv\",\n","#     \"/kaggle/input/csv-wesad/subject_5_data.csv\",\n","#     \"/kaggle/input/csv-wesad/subject_6_data.csv\",\n","#     \"/kaggle/input/csv-wesad/subject_7_data.csv\",\n","#     \"/kaggle/input/csv-wesad/subject_8_data.csv\",\n","#     \"/kaggle/input/csv-wesad/subject_9_data.csv\",\n","#     \"/kaggle/input/csv-wesad/subject_10_data.csv\",\n","#     \"/kaggle/input/csv-wesad/subject_11_data.csv\",\n","#     \"/kaggle/input/csv-wesad/subject_13_data.csv\",\n","#     \"/kaggle/input/csv-wesad/subject_14_data.csv\",\n","#     \"/kaggle/input/csv-wesad/subject_15_data.csv\",\n","#     \"/kaggle/input/csv-wesad/subject_16_data.csv\",\n","#     \"/kaggle/input/csv-wesad/subject_17_data.csv\",\n","# ]\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-20T12:12:40.151704Z","iopub.status.busy":"2024-01-20T12:12:40.151059Z"},"id":"UF8L-4keGwqc","trusted":true},"outputs":[],"source":["merged_data = pd.read_csv(\"/kaggle/input/wesad-merged-csv/merged_data.csv\")\n","# Assuming the last column contains the target labels\n","X = merged_data.iloc[:, :-1].values\n","y = merged_data.iloc[:, -1].values\n","\n","# Reshape the input data to fit the Conv1D model\n","X = X.reshape(X.shape[0], X.shape[1], 1)\n","\n","# Create time series split\n","tscv = TimeSeriesSplit(n_splits=2)\n","\n","# Create the model outside the loop\n","model = Sequential()\n","\n","# CNN layers\n","model.add(Conv1D(filters=64, kernel_size=3, activation='relu', padding='same', input_shape=(X.shape[1], 1)))\n","model.add(MaxPooling1D(pool_size=2))\n","model.add(Conv1D(filters=64, kernel_size=3, activation='relu', padding='same'))\n","model.add(MaxPooling1D(pool_size=2))\n","# model.add(Flatten())\n","\n","# LSTM layers with dropout\n","model.add(LSTM(64, return_sequences=True))\n","model.add(Dropout(0.3))\n","model.add(LSTM(64, return_sequences=False))\n","model.add(Dropout(0.3))\n","\n","# Fully connected layers\n","model.add(Dense(64, activation='relu'))\n","model.add(Dropout(0.3))\n","model.add(Dense(8, activation='softmax'))\n","\n","# Compile the model only once\n","model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","# Train and evaluate the model for each split\n","for train_index, test_index in tscv.split(X):\n","    X_train, X_test = X[train_index], X[test_index]\n","    y_train, y_test = y[train_index], y[test_index]\n","\n","    # Train the model\n","    history = model.fit(X_train, y_train, epochs=25, batch_size=512, validation_data=(X_test, y_test))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Plot training & validation accuracy values\n","plt.figure(figsize=(12, 6))\n","plt.plot(history.history['accuracy'])\n","plt.plot(history.history['val_accuracy'])\n","plt.title('Model accuracy')\n","plt.xlabel('Epoch')\n","plt.ylabel('Accuracy')\n","plt.legend(['Train', 'Validation'], loc='upper left')\n","plt.show()\n","\n","# Plot training & validation loss values\n","plt.figure(figsize=(12, 6))\n","plt.plot(history.history['loss'])\n","plt.plot(history.history['val_loss'])\n","plt.title('Model loss')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.legend(['Train', 'Validation'], loc='upper left')\n","plt.show()"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4216637,"sourceId":7273429,"sourceType":"datasetVersion"}],"dockerImageVersionId":30627,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
